{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab84edef-80e6-4834-9d96-e49bf9927fd5",
   "metadata": {},
   "source": [
    "#### Week 4 Project\n",
    " AI-Powered Data Analysis & Automation\n",
    " NO WORD COUNT\n",
    "#### Objective:\n",
    " This project aims to apply AI-driven techniques for data cleaning,\n",
    " visualization, predictive analytics, and automation. Participants will\n",
    " use AI tools such as Power BI, Google AutoML, and Python to\n",
    " analyze data, generate insights, and enhance business decision\n",
    "making.\n",
    "Tools Needed\n",
    " Google AutoML – For AI-powered data preprocessing and\n",
    " predictive modeling\n",
    " Power BI – For AI-driven data visualization and automated\n",
    " insights\n",
    " Python (Pandas, Scikit-learn, Matplotlib, Seaborn) – For\n",
    " advanced data analysis and modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b1d1d-8afa-4e5b-aff6-830adc24f4a1",
   "metadata": {},
   "source": [
    "#### Task 1: AI-Powered Data Cleaning and Preprocessing\n",
    " This task involves cleaning and preparing the dataset using AI tools to\n",
    " handle missing values, detect outliers, and ensure data consistency.\n",
    " Steps to Follow:\n",
    " Step 1: Upload the Dataset\n",
    " 1) Download the dataset from the provided source.\n",
    " 2) Open Google AutoML or Power BI.\n",
    " 3) Upload the dataset into the tool:\n",
    " Google AutoML: Click on \"New Dataset,\" select the file, and\n",
    " upload it.\n",
    " Power BI: Open Power BI, go to \"Home\" → \"Get Data\" →\n",
    " \"Excel/CSV\" → Select the dataset and click \"Load.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a099e-e087-4be7-ab54-895856c3796a",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba928d5-8658-49b9-aa4e-bc97aa57dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Here are the first 5 rows:\n",
      "   Customer_ID  Age  Gender    Income  Spending_Score  Credit_Score  \\\n",
      "0            1   56  Female  142418.0               7         391.0   \n",
      "1            2   69    Male   63088.0              82         652.0   \n",
      "2            3   46    Male  136868.0              91         662.0   \n",
      "3            4   32  Female       NaN              34         644.0   \n",
      "4            5   60    Male   59811.0              91         469.0   \n",
      "\n",
      "   Loan_Amount  Previous_Defaults  Marketing_Spend  Purchase_Frequency  \\\n",
      "0       8083.0                  1            15376                   3   \n",
      "1      34328.0                  2             6889                   6   \n",
      "2      47891.0                  2             6054                  29   \n",
      "3      25103.0                  2             4868                   8   \n",
      "4      44891.0                  1            17585                  12   \n",
      "\n",
      "  Seasonality  Sales  Customer_Churn  Defaulted  \n",
      "0         Low  32526               0          0  \n",
      "1         Low  78493               0          0  \n",
      "2      Medium  57198               1          0  \n",
      "3      Medium  48395               0          0  \n",
      "4        High  29031               1          0  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Customer_ID         500 non-null    int64  \n",
      " 1   Age                 500 non-null    int64  \n",
      " 2   Gender              500 non-null    object \n",
      " 3   Income              450 non-null    float64\n",
      " 4   Spending_Score      500 non-null    int64  \n",
      " 5   Credit_Score        450 non-null    float64\n",
      " 6   Loan_Amount         450 non-null    float64\n",
      " 7   Previous_Defaults   500 non-null    int64  \n",
      " 8   Marketing_Spend     500 non-null    int64  \n",
      " 9   Purchase_Frequency  500 non-null    int64  \n",
      " 10  Seasonality         500 non-null    object \n",
      " 11  Sales               500 non-null    int64  \n",
      " 12  Customer_Churn      500 non-null    int64  \n",
      " 13  Defaulted           500 non-null    int64  \n",
      "dtypes: float64(3), int64(9), object(2)\n",
      "memory usage: 54.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset from a CSV file\n",
    "file_path = \"raw_dataset_week4.csv\"  # Replace with actual file path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to confirm successful loading\n",
    "print(\"Dataset loaded successfully. Here are the first 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic info to check for missing values and data types\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e3774-8da9-4b10-92e9-8bd67d3e826c",
   "metadata": {},
   "source": [
    "#### Step 2: Handle Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85cf7708-0be3-4797-ac68-92d02ae7990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "Customer_ID            0\n",
      "Age                    0\n",
      "Gender                 0\n",
      "Income                50\n",
      "Spending_Score         0\n",
      "Credit_Score          50\n",
      "Loan_Amount           50\n",
      "Previous_Defaults      0\n",
      "Marketing_Spend        0\n",
      "Purchase_Frequency     0\n",
      "Seasonality            0\n",
      "Sales                  0\n",
      "Customer_Churn         0\n",
      "Defaulted              0\n",
      "dtype: int64\n",
      "Filled 'Income' missing values with mean: 84398.06\n",
      "Filled 'Credit_Score' missing values with mean: 573.41\n",
      "Filled 'Loan_Amount' missing values with mean: 28456.93\n",
      "\n",
      "Missing values after cleaning:\n",
      "Customer_ID           0\n",
      "Age                   0\n",
      "Gender                0\n",
      "Income                0\n",
      "Spending_Score        0\n",
      "Credit_Score          0\n",
      "Loan_Amount           0\n",
      "Previous_Defaults     0\n",
      "Marketing_Spend       0\n",
      "Purchase_Frequency    0\n",
      "Seasonality           0\n",
      "Sales                 0\n",
      "Customer_Churn        0\n",
      "Defaulted             0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows after handling missing values:\n",
      "   Customer_ID  Age  Gender         Income  Spending_Score  Credit_Score  \\\n",
      "0            1   56  Female  142418.000000               7         391.0   \n",
      "1            2   69    Male   63088.000000              82         652.0   \n",
      "2            3   46    Male  136868.000000              91         662.0   \n",
      "3            4   32  Female   84398.055556              34         644.0   \n",
      "4            5   60    Male   59811.000000              91         469.0   \n",
      "\n",
      "   Loan_Amount  Previous_Defaults  Marketing_Spend  Purchase_Frequency  \\\n",
      "0       8083.0                  1            15376                   3   \n",
      "1      34328.0                  2             6889                   6   \n",
      "2      47891.0                  2             6054                  29   \n",
      "3      25103.0                  2             4868                   8   \n",
      "4      44891.0                  1            17585                  12   \n",
      "\n",
      "  Seasonality  Sales  Customer_Churn  Defaulted  \n",
      "0         Low  32526               0          0  \n",
      "1         Low  78493               0          0  \n",
      "2      Medium  57198               1          0  \n",
      "3      Medium  48395               0          0  \n",
      "4        High  29031               1          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "# Reload the dataset to ensure we start with the original data\n",
    "file_path = \"raw_dataset_week4.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check initial missing values (should show 50 for Income, Credit_Score, Loan_Amount)\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with column means for numerical columns with NaN\n",
    "columns_with_missing = [\"Income\", \"Credit_Score\", \"Loan_Amount\"]\n",
    "for col in columns_with_missing:\n",
    "    mean_value = df[col].mean()\n",
    "    df[col] = df[col].fillna(mean_value)  # Direct assignment to avoid inplace warning\n",
    "    print(f\"Filled '{col}' missing values with mean: {mean_value:.2f}\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display the first few rows to confirm changes\n",
    "print(\"\\nFirst 5 rows after handling missing values:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optionally save the updated dataset (uncomment to save)\n",
    "# df.to_csv(\"cleaned_data_step2.csv\", index=False)\n",
    "# print(\"\\nDataset saved as 'cleaned_data_step2.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f13e6-86ed-40a2-b4a7-ed24372ac1a9",
   "metadata": {},
   "source": [
    "Observations for Step 2: Handle Missing Values\n",
    "Dataset State Before Cleaning:\n",
    "Missing Values Identified: \n",
    "Income: 50 missing values.\n",
    "\n",
    "Credit_Score: 50 missing values.\n",
    "\n",
    "Loan_Amount: 50 missing values.\n",
    "\n",
    "All other columns (Customer_ID, Age, Gender, Spending_Score, Previous_Defaults, Marketing_Spend, Purchase_Frequency, Seasonality, Sales, Customer_Churn, Defaulted): 0 missing values.\n",
    "\n",
    "Total Missing Values: 150 (50 per affected column), representing 10% of the 500 entries in each of the three columns.\n",
    "\n",
    "Implication: The dataset was incomplete for key financial metrics (Income, Credit_Score, Loan_Amount), which could impact analysis or modeling if not addressed.\n",
    "\n",
    "Cleaning Process:\n",
    "Method Applied: Missing values in Income, Credit_Score, and Loan_Amount were filled with their respective column means, calculated from the 450 non-null entries in each column.\n",
    "\n",
    "Mean Values Used:\n",
    "Income: 84,398.06 (average income across non-missing entries).\n",
    "\n",
    "Credit_Score: 573.41 (average credit score).\n",
    "\n",
    "Loan_Amount: 28,456.93 (average loan amount).\n",
    "\n",
    "Execution: The code reloaded the original dataset and applied the mean imputation successfully, ensuring the \"before cleaning\" state accurately reflects the initial 50 missing values per column.\n",
    "\n",
    "Dataset State After Cleaning:\n",
    "Missing Values Eliminated: \n",
    "All columns now show 0 missing values, confirming that the imputation filled all 150 gaps.\n",
    "\n",
    "First 5 Rows:\n",
    "Customer 4’s Income, originally missing, is now 84398.055556 (the mean), while Credit_Score (644.0) and Loan_Amount (25103.0) were already present.\n",
    "\n",
    "Other rows (e.g., Customers 1, 2, 3, 5) retain their original values, as they had no missing data in these columns.\n",
    "\n",
    "Data Integrity: The structure remains intact with 500 rows and 14 columns, and no unintended changes occurred to non-missing values.\n",
    "\n",
    "Key Observations:\n",
    "Effectiveness: The mean imputation successfully resolved all missing values, making the dataset complete for further analysis.\n",
    "\n",
    "Mean Values:\n",
    "Income mean of 84,398.06 suggests a moderate average income, with some variability (original values range from ~20,000 to ~149,000).\n",
    "\n",
    "Credit_Score mean of 573.41 is reasonable for a typical credit score range (300–850), indicating a generally creditworthy population.\n",
    "\n",
    "Loan_Amount mean of 28,456.93 reflects a moderate average loan size, with observed values ranging from ~5,000 to ~49,000.\n",
    "\n",
    "Impact on Data: Filling with means preserves the central tendency but may slightly reduce variance in these columns, potentially masking extreme cases. This is a trade-off of mean imputation versus other methods (e.g., median or interpolation).\n",
    "\n",
    "Consistency: The output aligns with expectations—Customer 4’s missing Income is now filled, and the rest of the data remains unchanged, validating the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce4248-875d-458f-9676-fe2d122b380d",
   "metadata": {},
   "source": [
    "#### Step 3: Detect and Handle Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b89d3ca-adcc-4c81-a4ff-99535ba90df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bounds for outliers:\n",
      "Income               -46872.375\n",
      "Spending_Score          -51.125\n",
      "Credit_Score            109.125\n",
      "Loan_Amount           -8426.000\n",
      "Previous_Defaults        -3.000\n",
      "Marketing_Spend       -7545.875\n",
      "Purchase_Frequency      -14.500\n",
      "Sales                -41516.875\n",
      "dtype: float64\n",
      "\n",
      "Upper bounds for outliers:\n",
      "Income                216110.625\n",
      "Spending_Score           153.875\n",
      "Credit_Score            1026.125\n",
      "Loan_Amount            64850.000\n",
      "Previous_Defaults          5.000\n",
      "Marketing_Spend        28687.125\n",
      "Purchase_Frequency        45.500\n",
      "Sales                 150548.125\n",
      "dtype: float64\n",
      "\n",
      "Original dataset shape: (500, 14)\n",
      "Cleaned dataset shape after removing outliers: (500, 14)\n",
      "\n",
      "Number of rows removed: 0\n",
      "\n",
      "First 5 rows of cleaned dataset:\n",
      "   Customer_ID  Age  Gender         Income  Spending_Score  Credit_Score  \\\n",
      "0            1   56  Female  142418.000000               7         391.0   \n",
      "1            2   69    Male   63088.000000              82         652.0   \n",
      "2            3   46    Male  136868.000000              91         662.0   \n",
      "3            4   32  Female   84398.055556              34         644.0   \n",
      "4            5   60    Male   59811.000000              91         469.0   \n",
      "\n",
      "   Loan_Amount  Previous_Defaults  Marketing_Spend  Purchase_Frequency  \\\n",
      "0       8083.0                  1            15376                   3   \n",
      "1      34328.0                  2             6889                   6   \n",
      "2      47891.0                  2             6054                  29   \n",
      "3      25103.0                  2             4868                   8   \n",
      "4      44891.0                  1            17585                  12   \n",
      "\n",
      "  Seasonality  Sales  Customer_Churn  Defaulted  \n",
      "0         Low  32526               0          0  \n",
      "1         Low  78493               0          0  \n",
      "2      Medium  57198               1          0  \n",
      "3      Medium  48395               0          0  \n",
      "4        High  29031               1          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Detect and handle outliers using IQR method\n",
    "# Select numerical columns for outlier detection\n",
    "numerical_columns = [\"Income\", \"Spending_Score\", \"Credit_Score\", \"Loan_Amount\", \n",
    "                     \"Previous_Defaults\", \"Marketing_Spend\", \"Purchase_Frequency\", \"Sales\"]\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for each numerical column\n",
    "Q1 = df[numerical_columns].quantile(0.25)\n",
    "Q3 = df[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print bounds for reference\n",
    "print(\"Lower bounds for outliers:\")\n",
    "print(lower_bound)\n",
    "print(\"\\nUpper bounds for outliers:\")\n",
    "print(upper_bound)\n",
    "\n",
    "# Remove rows with values outside the bounds (outliers)\n",
    "df_cleaned = df[~((df[numerical_columns] < lower_bound) | (df[numerical_columns] > upper_bound)).any(axis=1)]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nOriginal dataset shape:\", df.shape)\n",
    "print(\"Cleaned dataset shape after removing outliers:\", df_cleaned.shape)\n",
    "print(\"\\nNumber of rows removed:\", df.shape[0] - df_cleaned.shape[0])\n",
    "\n",
    "# Display first 5 rows of cleaned dataset\n",
    "print(\"\\nFirst 5 rows of cleaned dataset:\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6552f2-cc51-4a63-80ef-360805dc6f44",
   "metadata": {},
   "source": [
    "#### Process Overview\n",
    "Method: The Interquartile Range (IQR) method was used to detect outliers in eight numerical columns: Income, Spending_Score, Credit_Score, Loan_Amount, Previous_Defaults, Marketing_Spend, Purchase_Frequency, and Sales.\n",
    "\n",
    "Steps: \n",
    "Calculated Q1 (25th percentile) and Q3 (75th percentile) for each column.\n",
    "\n",
    "Computed IQR = Q3 - Q1.\n",
    "\n",
    "Defined outlier bounds as:\n",
    "Lower bound = Q1 - 1.5 * IQR.\n",
    "\n",
    "Upper bound = Q3 + 1.5 * IQR.\n",
    "\n",
    "Removed rows where any value fell outside these bounds.\n",
    "\n",
    "#### Outlier Bounds\n",
    "Lower Bounds:\n",
    "Income: -46,872.375\n",
    "\n",
    "Spending_Score: -51.125\n",
    "\n",
    "Credit_Score: 109.125\n",
    "\n",
    "Loan_Amount: -8,426.000\n",
    "\n",
    "Previous_Defaults: -3.000\n",
    "\n",
    "Marketing_Spend: -7,545.875\n",
    "\n",
    "Purchase_Frequency: -14.500\n",
    "\n",
    "Sales: -41,516.875\n",
    "\n",
    "Upper Bounds:\n",
    "Income: 216,110.625\n",
    "\n",
    "Spending_Score: 153.875\n",
    "\n",
    "Credit_Score: 1,026.125\n",
    "\n",
    "Loan_Amount: 64,850.000\n",
    "\n",
    "Previous_Defaults: 5.000\n",
    "\n",
    "Marketing_Spend: 28,687.125\n",
    "\n",
    "Purchase_Frequency: 45.500\n",
    "\n",
    "Sales: 150,548.125\n",
    "\n",
    "Results\n",
    "Original Shape: 500 rows, 14 columns.\n",
    "\n",
    "Cleaned Shape: 500 rows, 14 columns.\n",
    "\n",
    "Rows Removed: 0 (no change in dataset size).\n",
    "\n",
    "First 5 Rows: Identical to the post-Step 2 dataset, with no alterations.\n",
    "\n",
    "#### Key Observations\n",
    "No Outliers Detected:\n",
    "Despite applying the IQR method, no rows were flagged as outliers, meaning all 500 rows had values within the calculated bounds for all eight numerical columns.\n",
    "\n",
    "This suggests the dataset is unusually uniform or that the IQR bounds are wide enough to encompass all observed values.\n",
    "\n",
    "Bounds Analysis:\n",
    "Income (-46,872.375 to 216,110.625): \n",
    "Observed range (~20,000 to ~149,000) fits well within this wide range. Even extremes like 149,922 (Customer 196) are below 216,110.625.\n",
    "\n",
    "Spending_Score (-51.125 to 153.875): \n",
    "Range is 1 to 99 (e.g., Customer 37 has 1, Customer 147 has 99). Since the lower bound is negative (impossible) and the upper bound exceeds 100, no outliers are possible here.\n",
    "\n",
    "Credit_Score (109.125 to 1,026.125): \n",
    "Range (~300 to ~848) is fully contained. Even 848 (Customer 194) is below 1,026.125.\n",
    "\n",
    "Loan_Amount (-8,426.000 to 64,850.000): \n",
    "Range (~5,000 to ~49,000) fits comfortably. No negative loans exist, and 49,936 (Customer 116) is below 64,850.\n",
    "\n",
    "Previous_Defaults (-3.000 to 5.000): \n",
    "Values are 0, 1, or 2—well within bounds (negative is impossible).\n",
    "\n",
    "Marketing_Spend (-7,545.875 to 28,687.125): \n",
    "Range (~1,234 to ~19,990) fits, with 19,990 (Customer 71) below 28,687.125.\n",
    "\n",
    "Purchase_Frequency (-14.500 to 45.500): \n",
    "Range (1 to 29) is contained, with no negative values possible.\n",
    "\n",
    "Sales (-41,516.875 to 150,548.125): \n",
    "Range (~5,000 to ~99,835) fits, with 99,835 (Customer 169) below 150,548.125.\n",
    "\n",
    "Why No Rows Removed?:\n",
    "The IQR bounds are quite broad due to the dataset’s natural variability (e.g., Income IQR spans from ~50,000 to ~130,000, extended by 1.5x).\n",
    "\n",
    "All observed values fall within these generous ranges, indicating either:\n",
    "The data is well-behaved with no extreme outliers.\n",
    "\n",
    "The 1.5 * IQR threshold is too lenient for this dataset’s distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc23470-fade-43d3-becc-5061796b9ed5",
   "metadata": {},
   "source": [
    "#### Step 4: Save the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "428e6564-f29e-4b66-9e94-40681a8bad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully as 'cleaned_data'\n",
      "Final dataset shape: (500, 14)\n",
      "\n",
      "First 5 rows of the saved dataset:\n",
      "   Customer_ID  Age  Gender         Income  Spending_Score  Credit_Score  \\\n",
      "0            1   56  Female  142418.000000               7         391.0   \n",
      "1            2   69    Male   63088.000000              82         652.0   \n",
      "2            3   46    Male  136868.000000              91         662.0   \n",
      "3            4   32  Female   84398.055556              34         644.0   \n",
      "4            5   60    Male   59811.000000              91         469.0   \n",
      "\n",
      "   Loan_Amount  Previous_Defaults  Marketing_Spend  Purchase_Frequency  \\\n",
      "0       8083.0                  1            15376                   3   \n",
      "1      34328.0                  2             6889                   6   \n",
      "2      47891.0                  2             6054                  29   \n",
      "3      25103.0                  2             4868                   8   \n",
      "4      44891.0                  1            17585                  12   \n",
      "\n",
      "  Seasonality  Sales  Customer_Churn  Defaulted  \n",
      "0         Low  32526               0          0  \n",
      "1         Low  78493               0          0  \n",
      "2      Medium  57198               1          0  \n",
      "3      Medium  48395               0          0  \n",
      "4        High  29031               1          0  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the cleaned dataset\n",
    "output_file = \"cleaned_data\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# Confirm the save\n",
    "print(f\"Cleaned dataset saved successfully as '{output_file}'\")\n",
    "print(\"Final dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the saved dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d4533-968c-4ff5-af5d-3e732bc584c7",
   "metadata": {},
   "source": [
    "Final check after Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a876c82-c8ff-48c6-896f-f28c8a61eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing/Null Values Check:\n",
      "Customer_ID           0\n",
      "Age                   0\n",
      "Gender                0\n",
      "Income                0\n",
      "Spending_Score        0\n",
      "Credit_Score          0\n",
      "Loan_Amount           0\n",
      "Previous_Defaults     0\n",
      "Marketing_Spend       0\n",
      "Purchase_Frequency    0\n",
      "Seasonality           0\n",
      "Sales                 0\n",
      "Customer_Churn        0\n",
      "Defaulted             0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      "Customer_ID             int64\n",
      "Age                     int64\n",
      "Gender                 object\n",
      "Income                float64\n",
      "Spending_Score          int64\n",
      "Credit_Score          float64\n",
      "Loan_Amount           float64\n",
      "Previous_Defaults       int64\n",
      "Marketing_Spend         int64\n",
      "Purchase_Frequency      int64\n",
      "Seasonality            object\n",
      "Sales                   int64\n",
      "Customer_Churn          int64\n",
      "Defaulted               int64\n",
      "dtype: object\n",
      "\n",
      "Basic Consistency Checks:\n",
      "Number of negative values in 'Age': 0\n",
      "Number of negative values in 'Income': 0\n",
      "Number of negative values in 'Spending_Score': 0\n",
      "Number of negative values in 'Credit_Score': 0\n",
      "Number of negative values in 'Loan_Amount': 0\n",
      "Number of negative values in 'Previous_Defaults': 0\n",
      "Number of negative values in 'Marketing_Spend': 0\n",
      "Number of negative values in 'Purchase_Frequency': 0\n",
      "Number of negative values in 'Sales': 0\n",
      "\n",
      "Gender Unique Values: ['Female' 'Male']\n",
      "Seasonality Unique Values: ['Low' 'Medium' 'High']\n",
      "Customer_Churn Unique Values: [0 1]\n",
      "Defaulted Unique Values: [0 1]\n",
      "\n",
      "Summary Statistics:\n",
      "              Age         Income  Spending_Score  Credit_Score   Loan_Amount  \\\n",
      "count  500.000000     500.000000      500.000000    500.000000    500.000000   \n",
      "mean    44.220000   84398.055556       50.862000    573.411111  28456.928889   \n",
      "std     15.036082   38049.398377       29.125101    149.302942  11788.254534   \n",
      "min     18.000000   20055.000000        1.000000    300.000000   5163.000000   \n",
      "25%     32.000000   51746.250000       25.750000    453.000000  19052.500000   \n",
      "50%     45.000000   84398.055556       51.000000    573.411111  28456.928889   \n",
      "75%     57.000000  117492.000000       77.000000    682.250000  37371.500000   \n",
      "max     69.000000  149922.000000       99.000000    848.000000  49936.000000   \n",
      "\n",
      "       Previous_Defaults  Marketing_Spend  Purchase_Frequency         Sales  \n",
      "count          500.00000       500.000000          500.000000    500.000000  \n",
      "mean             0.97400     10558.128000           15.350000  54378.954000  \n",
      "std              0.82625      5508.219008            8.475327  27263.106468  \n",
      "min              0.00000      1024.000000            1.000000   5203.000000  \n",
      "25%              0.00000      6041.500000            8.000000  30507.500000  \n",
      "50%              1.00000     10754.000000           16.000000  54032.500000  \n",
      "75%              2.00000     15099.750000           23.000000  78523.750000  \n",
      "max              2.00000     19990.000000           29.000000  99835.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO  # Import StringIO from io module\n",
    "\n",
    "# Load the cleaned dataset\n",
    "# Option 1: If checking the saved file directly (recommended for your local environment)\n",
    "file_path = \"cleaned_data.csv\"  # Adjust to your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing/null values\n",
    "print(\"Missing/Null Values Check:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Basic consistency checks\n",
    "print(\"\\nBasic Consistency Checks:\")\n",
    "# Check for negative values where they shouldn’t exist\n",
    "numerical_cols = [\"Age\", \"Income\", \"Spending_Score\", \"Credit_Score\", \"Loan_Amount\", \n",
    "                  \"Previous_Defaults\", \"Marketing_Spend\", \"Purchase_Frequency\", \"Sales\"]\n",
    "for col in numerical_cols:\n",
    "    negatives = df[df[col] < 0].shape[0]\n",
    "    print(f\"Number of negative values in '{col}': {negatives}\")\n",
    "\n",
    "# Check categorical columns for unexpected values\n",
    "print(\"\\nGender Unique Values:\", df[\"Gender\"].unique())\n",
    "print(\"Seasonality Unique Values:\", df[\"Seasonality\"].unique())\n",
    "print(\"Customer_Churn Unique Values:\", df[\"Customer_Churn\"].unique())\n",
    "print(\"Defaulted Unique Values:\", df[\"Defaulted\"].unique())\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb433f1-ec12-4e76-9a3e-db611f697158",
   "metadata": {},
   "source": [
    "#### Results Summary\n",
    "Missing/Null Values  \n",
    "Finding: No missing or null values detected across all 14 columns (500 rows each).\n",
    "\n",
    "Details: Customer_ID, Age, Gender, Income, Spending_Score, Credit_Score, Loan_Amount, Previous_Defaults, Marketing_Spend, Purchase_Frequency, Seasonality, Sales, Customer_Churn, and Defaulted all show 0 nulls.\n",
    "\n",
    "Conclusion: Step 2’s mean imputation successfully filled all 150 original missing values.\n",
    "\n",
    "Data Types  \n",
    "Finding: Data types are consistent and appropriate.\n",
    "\n",
    "Details: \n",
    "Integers (int64): Customer_ID, Age, Spending_Score, Previous_Defaults, Marketing_Spend, Purchase_Frequency, Sales, Customer_Churn, Defaulted.\n",
    "\n",
    "Floats (float64): Income, Credit_Score, Loan_Amount (due to mean imputation).\n",
    "\n",
    "Objects (object): Gender, Seasonality (categorical).\n",
    "\n",
    "Conclusion: Types align with data content; float precision from imputation is expected.\n",
    "\n",
    "Consistency Checks  \n",
    "Negative Values: No negative values found in Age, Income, Spending_Score, Credit_Score, Loan_Amount, Previous_Defaults, Marketing_Spend, Purchase_Frequency, or Sales.\n",
    "\n",
    "Categorical Values: \n",
    "Gender: Only \"Female\" and \"Male\".\n",
    "\n",
    "Seasonality: Only \"Low\", \"Medium\", \"High\".\n",
    "\n",
    "Customer_Churn and Defaulted: Only 0 and 1.\n",
    "\n",
    "Conclusion: No illogical or unexpected values detected; data adheres to expected ranges and categories.\n",
    "\n",
    "#### Summary Statistics  \n",
    "Key Metrics:\n",
    "Age: Mean 44.22, Min 18, Max 69.\n",
    "\n",
    "Income: Mean 84,398.06, Min 20,055, Max 149,922, Std 38,049.40 (imputation centralized some values).\n",
    "\n",
    "Credit_Score: Mean 573.41, Min 300, Max 848, Std 149.30.\n",
    "\n",
    "Loan_Amount: Mean 28,456.93, Min 5,163, Max 49,936, Std 11,788.25.\n",
    "\n",
    "Sales: Mean 54,378.95, Min 5,203, Max 99,835, Std 27,263.11.\n",
    "\n",
    "Observation: Ranges are reasonable; means reflect imputation (e.g., Income median = mean due to 50 imputed values). No outliers were removed (Step 3), so extremes persist (e.g., Income 149,922).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4978f-30ed-4490-9d2d-23a70e608c25",
   "metadata": {},
   "source": [
    "#### Task 3: AI-Driven Predictive and Prescriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06399b7c-6222-4ef5-b29b-1542052eba65",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate Model Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef902010-a385-4977-805c-85fc575e084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned unique values in 'Seasonality': ['Low' 'Medium' 'High']\n",
      "Value counts:\n",
      " Seasonality\n",
      "Medium    169\n",
      "High      168\n",
      "Low       163\n",
      "Name: count, dtype: int64\n",
      "Features (X) columns: ['Age', 'Income', 'Spending_Score', 'Credit_Score', 'Loan_Amount', 'Previous_Defaults', 'Marketing_Spend', 'Purchase_Frequency', 'Seasonality_High', 'Seasonality_Medium', 'Spend_x_Freq', 'Score_x_Income', 'Marketing_Spend', 'Purchase_Frequency', 'Spending_Score', 'Income', 'Marketing_Spend^2', 'Marketing_Spend Purchase_Frequency', 'Marketing_Spend Spending_Score', 'Marketing_Spend Income', 'Purchase_Frequency^2', 'Purchase_Frequency Spending_Score', 'Purchase_Frequency Income', 'Spending_Score^2', 'Spending_Score Income', 'Income^2']\n",
      "Sample X data:\n",
      "    Age         Income  Spending_Score  Credit_Score  Loan_Amount  \\\n",
      "0   56  142418.000000               7         391.0       8083.0   \n",
      "1   69   63088.000000              82         652.0      34328.0   \n",
      "2   46  136868.000000              91         662.0      47891.0   \n",
      "3   32   84398.055556              34         644.0      25103.0   \n",
      "4   60   59811.000000              91         469.0      44891.0   \n",
      "\n",
      "   Previous_Defaults  Marketing_Spend  Purchase_Frequency  Seasonality_High  \\\n",
      "0                  1            15376                   3             False   \n",
      "1                  2             6889                   6             False   \n",
      "2                  2             6054                  29             False   \n",
      "3                  2             4868                   8             False   \n",
      "4                  1            17585                  12              True   \n",
      "\n",
      "   Seasonality_Medium  ...  Marketing_Spend^2  \\\n",
      "0               False  ...        236421376.0   \n",
      "1               False  ...         47458321.0   \n",
      "2                True  ...         36650916.0   \n",
      "3                True  ...         23697424.0   \n",
      "4               False  ...        309232225.0   \n",
      "\n",
      "   Marketing_Spend Purchase_Frequency  Marketing_Spend Spending_Score  \\\n",
      "0                             46128.0                        107632.0   \n",
      "1                             41334.0                        564898.0   \n",
      "2                            175566.0                        550914.0   \n",
      "3                             38944.0                        165512.0   \n",
      "4                            211020.0                       1600235.0   \n",
      "\n",
      "   Marketing_Spend Income  Purchase_Frequency^2  \\\n",
      "0            2.189819e+09                   9.0   \n",
      "1            4.346132e+08                  36.0   \n",
      "2            8.285989e+08                 841.0   \n",
      "3            4.108497e+08                  64.0   \n",
      "4            1.051776e+09                 144.0   \n",
      "\n",
      "   Purchase_Frequency Spending_Score  Purchase_Frequency Income  \\\n",
      "0                               21.0               4.272540e+05   \n",
      "1                              492.0               3.785280e+05   \n",
      "2                             2639.0               3.969172e+06   \n",
      "3                              272.0               6.751844e+05   \n",
      "4                             1092.0               7.177320e+05   \n",
      "\n",
      "   Spending_Score^2  Spending_Score Income      Income^2  \n",
      "0              49.0           9.969260e+05  2.028289e+10  \n",
      "1            6724.0           5.173216e+06  3.980096e+09  \n",
      "2            8281.0           1.245499e+07  1.873285e+10  \n",
      "3            1156.0           2.869534e+06  7.123032e+09  \n",
      "4            8281.0           5.442801e+06  3.577356e+09  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Best XGBoost parameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}\n",
      "\n",
      "XGBoost Performance Evaluation:\n",
      "Mean Squared Error (MSE): 899290526.93\n",
      "Root Mean Squared Error (RMSE): 29988.17\n",
      "R-squared (R²): -0.11\n",
      "\n",
      "XGBoost Feature Importance:\n",
      "                               Feature  Importance\n",
      "18      Marketing_Spend Spending_Score    0.093473\n",
      "10                        Spend_x_Freq    0.087148\n",
      "21   Purchase_Frequency Spending_Score    0.085708\n",
      "22           Purchase_Frequency Income    0.076958\n",
      "5                    Previous_Defaults    0.075917\n",
      "7                   Purchase_Frequency    0.065444\n",
      "3                         Credit_Score    0.060585\n",
      "6                      Marketing_Spend    0.058423\n",
      "8                     Seasonality_High    0.057670\n",
      "4                          Loan_Amount    0.057275\n",
      "2                       Spending_Score    0.057133\n",
      "19              Marketing_Spend Income    0.056547\n",
      "11                      Score_x_Income    0.054664\n",
      "9                   Seasonality_Medium    0.050966\n",
      "1                               Income    0.039953\n",
      "0                                  Age    0.022136\n",
      "23                    Spending_Score^2    0.000000\n",
      "24               Spending_Score Income    0.000000\n",
      "20                Purchase_Frequency^2    0.000000\n",
      "13                  Purchase_Frequency    0.000000\n",
      "17  Marketing_Spend Purchase_Frequency    0.000000\n",
      "16                   Marketing_Spend^2    0.000000\n",
      "15                              Income    0.000000\n",
      "14                      Spending_Score    0.000000\n",
      "12                     Marketing_Spend    0.000000\n",
      "25                            Income^2    0.000000\n",
      "\n",
      "Baseline (Mean) MSE: 813101997.6760441\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Clean Seasonality\n",
    "df['Seasonality'] = df['Seasonality'].str.strip().str.capitalize()\n",
    "print(\"Cleaned unique values in 'Seasonality':\", df['Seasonality'].unique())\n",
    "print(\"Value counts:\\n\", df['Seasonality'].value_counts())\n",
    "\n",
    "# Encode Seasonality, dropping 'Low' as baseline\n",
    "df_encoded = pd.get_dummies(df, columns=['Seasonality'])\n",
    "df_encoded = df_encoded.drop('Seasonality_Low', axis=1)\n",
    "\n",
    "# Cap extreme Sales values\n",
    "sales_cap = df_encoded['Sales'].quantile(0.95)\n",
    "df_encoded['Sales'] = df_encoded['Sales'].clip(upper=sales_cap)\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "numeric_features = ['Age', 'Income', 'Spending_Score', 'Credit_Score', 'Loan_Amount', \n",
    "                    'Previous_Defaults', 'Marketing_Spend', 'Purchase_Frequency']\n",
    "seasonality_cols = ['Seasonality_High', 'Seasonality_Medium']\n",
    "X = df_encoded[numeric_features + seasonality_cols].copy()\n",
    "\n",
    "# Add interaction features using .loc\n",
    "X.loc[:, 'Spend_x_Freq'] = X['Marketing_Spend'] * X['Purchase_Frequency']\n",
    "X.loc[:, 'Score_x_Income'] = X['Spending_Score'] * X['Income']\n",
    "\n",
    "# Add polynomial features (degree 2) for key predictors\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X[['Marketing_Spend', 'Purchase_Frequency', 'Spending_Score', 'Income']])\n",
    "poly_cols = poly.get_feature_names_out(['Marketing_Spend', 'Purchase_Frequency', 'Spending_Score', 'Income'])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_cols, index=X.index)\n",
    "\n",
    "# Combine original and polynomial features\n",
    "X = pd.concat([X, X_poly_df], axis=1)\n",
    "\n",
    "# Debug: Check features\n",
    "print(\"Features (X) columns:\", X.columns.tolist())\n",
    "print(\"Sample X data:\\n\", X.head())\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y = df_encoded['Sales']\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost with broader tuning\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.05, 0.1, 0.15]\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best XGBoost model\n",
    "best_model = grid_search_xgb.best_estimator_\n",
    "print(\"Best XGBoost parameters:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# Predictions\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nXGBoost Performance Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "})\n",
    "print(\"\\nXGBoost Feature Importance:\")\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "# Baseline check\n",
    "mean_sales = y_test.mean()\n",
    "baseline_mse = mean_squared_error(y_test, [mean_sales] * len(y_test))\n",
    "print(\"\\nBaseline (Mean) MSE:\", baseline_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b0633-2c4e-453b-8ada-b3e6e1394efd",
   "metadata": {},
   "source": [
    "Seasonality Cleaned Values:\n",
    "Cleaned unique values in 'Seasonality': ['Low', 'Medium', 'High']\n",
    "\n",
    "Value counts for 'Seasonality':\n",
    "\n",
    "Medium: 169\n",
    "\n",
    "High: 168\n",
    "\n",
    "Low: 163\n",
    "\n",
    "Feature Engineering:\n",
    "The dataset now includes several new interaction and polynomial features to capture complex relationships:\n",
    "\n",
    "Interaction features like Spend_x_Freq, Score_x_Income.\n",
    "\n",
    "Polynomial features for Marketing_Spend, Purchase_Frequency, Spending_Score, and Income.\n",
    "\n",
    "Model Performance:\n",
    "Best XGBoost Parameters:\n",
    "\n",
    "Learning Rate: 0.05\n",
    "\n",
    "Max Depth: 7\n",
    "\n",
    "Number of Estimators: 200\n",
    "\n",
    "Performance Metrics:\n",
    "\n",
    "Mean Squared Error (MSE): 899,290,526.93\n",
    "\n",
    "Root Mean Squared Error (RMSE): 29,988.17\n",
    "\n",
    "R-squared (R²): -0.11\n",
    "\n",
    "Baseline (Mean) MSE: 813,101,997.68\n",
    "\n",
    "Feature Importance:\n",
    "The most important features contributing to the model include:\n",
    "\n",
    "Marketing_Spend Spending_Score\n",
    "\n",
    "Spend_x_Freq\n",
    "\n",
    "Purchase_Frequency Spending_Score\n",
    "\n",
    "Purchase_Frequency Income\n",
    "\n",
    "Previous_Defaults\n",
    "\n",
    "Purchase_Frequency\n",
    "\n",
    "Credit_Score\n",
    "\n",
    "Key Insights:\n",
    "Negative R-squared (R²):\n",
    "\n",
    "The negative R² value (-0.11) indicates that the model is currently performing worse than a horizontal line representing the mean of the target variable (sales).\n",
    "\n",
    "This suggests that the current set of features and their transformations are not effectively capturing the relationships needed to predict sales.\n",
    "\n",
    "Model Accuracy:\n",
    "\n",
    "While the MSE and RMSE values provide a measure of prediction error, the R² value shows that the model requires further refinement to improve its predictive power.\n",
    "\n",
    "Feature Impact:\n",
    "\n",
    "The feature importance analysis reveals that interaction terms like Marketing_Spend Spending_Score and Spend_x_Freq are significant contributors.\n",
    "\n",
    "It's essential to explore further feature engineering or possibly remove less relevant features to enhance the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78757e-d980-4df7-8ed8-f32d29c3d5c4",
   "metadata": {},
   "source": [
    "#### Task 4: AI for Business Strategy and Risk Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1c5e7b-41b9-41be-a76a-f5c64510ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Selecting features and target variable\n",
    "X = df[['Income', 'Loan_Amount', 'Credit_Score']]\n",
    "y = df['Defaulted']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc501b84-75e7-46ed-9b57-9beb3c4be186",
   "metadata": {},
   "source": [
    "~ END ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
